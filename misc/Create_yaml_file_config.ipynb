{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "##                          yaml program options                     ##\n",
    "#######################################################################\n",
    "    \n",
    "# default program options for basic OBIS run - modify as desired in this section, \n",
    "# creates *yml file to supply as config for command line run\n",
    "# to use first activate python environment, then run as: python mypython.py options.yaml\n",
    "# T. Wellman, BCB Group, USGS\n",
    "\n",
    "import yaml, collections, csv, logging, pkg_resources\n",
    "\n",
    "   \n",
    "def default_inputs():\n",
    "\n",
    "    \n",
    "    # ScienceBase Item ID - search for data files \n",
    "    #--------------------------------------------\n",
    "    collection_id = '57fe93d5e4b0824b2d14cbe1'  # '579b64c6e4b0589fa1c98118' \n",
    "    \n",
    "    #\n",
    "    # Dictionary of ScienceBase search terms - use list format only, not case sensitive \n",
    "    #\n",
    "    file_srch = dict([('title', ['DarwinCore:event','DarwinCore:occurrence',\n",
    "                                'DarwinCore:measurementOrFact','final processed source',\n",
    "                                'Final Processed File']),\n",
    "                      ('name', ['occurrence', 'event', 'measurementOrFact'])])\n",
    "    file_srch['ftype_req'] = ['.csv', '.zip']\n",
    "\n",
    "\n",
    "    #--------------------------------\n",
    "    # Folder directories\n",
    "    #--------------------------------\n",
    "\n",
    "    #\n",
    "    # main directory to run program (None defaults to current directory)\n",
    "    #\n",
    "    workdir = None\n",
    "\n",
    "    #\n",
    "    # folder with original (source) csv files\n",
    "    #\n",
    "    source_data_dir = './source_data_test'\n",
    "\n",
    "    #\n",
    "    # folder with netCDF files (converted from source csv)\n",
    "    #\n",
    "    erddap_data_dir = './erddap_data/nc_store'\n",
    "\n",
    "    #\n",
    "    # Relative path to the netCDF files on the destination server, for the datasets.xml file.\n",
    "    #\n",
    "    server_nc_directory = './erddap_data/nc_store'\n",
    "\n",
    "    #\n",
    "    # folder with regenerated csv files (converted from netCDF)\n",
    "    #\n",
    "    recon_data_dir = erddap_data_dir + '/recreate_files'\n",
    "\n",
    "    #\n",
    "    # folder to store error and other report files \n",
    "    #\n",
    "    report_dir = erddap_data_dir + '/processing_reports'\n",
    "\n",
    "    #\n",
    "    # folder to store temporary files when data chunking active (dataframe option)\n",
    "    #\n",
    "    tempdir = erddap_data_dir + '/test/'\n",
    "    \n",
    "    #\n",
    "    # path + name (Darwin Core Standard) in json file, \"None\" to bypass\n",
    "    #\n",
    "    darwin_vocab = pkg_resources.resource_filename(__name__, 'data/DarwinCore_vocab.json')\n",
    "    vocab_name  = 'Darwin Core Standard'\n",
    "    \n",
    "    #--------------------------------\n",
    "    # Processing method (options)\n",
    "    #--------------------------------\n",
    "\n",
    "    # Exploratory approaches at processing tabular data\n",
    "    #  \"dataframe\" approach to process + convert csv files(xarray/dask/pandas) Tristan Wellman, USGS \n",
    "    #  \"messytables\" approach (csv_reader/messytables))  John Long, USGS\n",
    "    #\n",
    "    convert_method = \"dataframe\" # \"messytables\"  \n",
    "    \n",
    "\n",
    "    #--------------------------------\n",
    "    # processing flags (options)\n",
    "    #--------------------------------\n",
    "\n",
    "    #\n",
    "    # Whether to fetch metadata from ScienceBase.  If this is True and fetch_csvs\n",
    "    # is false, it will only fetch metadata.\n",
    "    #\n",
    "    fetch_metadata = True \n",
    "\n",
    "    #\n",
    "    # Whether to fetch source files from ScienceBase.\n",
    "    #\n",
    "    fetch_csvs = True\n",
    "\n",
    "    #\n",
    "    # Whether to create netCDF files from source csv files\n",
    "    #\n",
    "    create_netcdf_files = True\n",
    "\n",
    "    #\n",
    "    # Whether to create a datasets.xml from the netCDF files found in the erddap_data_dir\n",
    "    #\n",
    "    create_datasets_xml = True\n",
    "\n",
    "    #\n",
    "    # flag whether to overwrite source files.  If set to False, reprocessing input file will be skipped.\n",
    "    #\n",
    "    file_overwrite = False\n",
    "    \n",
    "    #\n",
    "    # flag whether to overwrite converted files.  If set to False, reprocessing input file will be skipped.\n",
    "    #\n",
    "    proc_overwrite = False\n",
    "\n",
    "    #\n",
    "    # flag whether to regenerate test csv files from netCDF, compare original (source) csv to regenerated csv\n",
    "    #\n",
    "    compare_csv2csv = True\n",
    "\n",
    "    #\n",
    "    # flag whether to delete regenerated csv file from netCDF, after testing \n",
    "    #\n",
    "    dump_csv = True\n",
    "\n",
    "    #\n",
    "    # flag whether to output comparison report file (if compare_csv2csv = True)\n",
    "    #\n",
    "    error_report = True\n",
    "\n",
    "    #\n",
    "    # flag to print comparison table summaries to screen\n",
    "    #\n",
    "    table_output = False\n",
    "\n",
    "    #\n",
    "    # maximum errors to report per data column in processing reports (one per file, if error_report active)\n",
    "    #\n",
    "    max_report = 50\n",
    "\n",
    "\n",
    "    #--------------------------------\n",
    "    # Messy table ONLY options\n",
    "    #--------------------------------\n",
    "\n",
    "    #\n",
    "    # Whether to create a single virtual dataset from multiple netCDFs that were created from a single csv\n",
    "    #\n",
    "    create_virtual_datasets = False\n",
    "\n",
    "    #\n",
    "    # Whether to turn on verbose logging\n",
    "    #\n",
    "    verbose = False\n",
    "\n",
    "    #\n",
    "    # Size in number of rows of the sample.  Only takes effect if sample is greater than zero.\n",
    "    #\n",
    "    sample_size = 0\n",
    "\n",
    "    #\n",
    "    # Number of rows to use to guess column type\n",
    "    #\n",
    "    window = 500\n",
    "\n",
    "    #\n",
    "    # Max number of rows per netCDF file.  If greater than zero, multiple files will be created if necessary.\n",
    "    # Zero Value signals to create a single netCDF file for the dataset.\n",
    "    #\n",
    "    rows_per_file = 0\n",
    "\n",
    "\n",
    "    #--------------------------------\n",
    "    # Dataframe ONLY options\n",
    "    #--------------------------------\n",
    "    \n",
    "    #\n",
    "    # force convert (date) variables to datetime objects \n",
    "    # 'datetime': iso datetime variable, 'string': iso string date, \"integer\": days since 1-1-1970 basedate, \n",
    "    # otherwise : do not convert)\n",
    "    #\n",
    "    date_convert = 'string' # 'datetime'\n",
    "    \n",
    "    #\n",
    "    # date format when forced to change (date_convert = True)\n",
    "    #\n",
    "    date_fmt = '%Y-%m-%dT%H:%M:%SZ' # \"%Y-%m-%d\"  \n",
    "\n",
    "    #\n",
    "    # file processing chunk size (number of elements, \"None\" deactivates chunking) \n",
    "    #\n",
    "    chunk_elements = 5e6\n",
    "    \n",
    "    #\n",
    "    # attempt CF compliancy standards\n",
    "    #\n",
    "    cf_comply = False\n",
    "    \n",
    "    #\n",
    "    # representation for missing string entries (if == '_FillValue', uses ' ' activates NetCDF _FillValue)\n",
    "    #\n",
    "    absent_string = 'NA'\n",
    "    \n",
    "    #\n",
    "    #  specify netcdf type: NETCDF4, NETCDF4_CLASSIC, NETCDF3_64BIT, or NETCDF3_CLASSIC\n",
    "    #\n",
    "    netcdf_type = 'NETCDF4_CLASSIC'  \n",
    "    \n",
    "    #--------------------------------\n",
    "    # file comparison options\n",
    "    #--------------------------------\n",
    "\n",
    "    #\n",
    "    # file encoding format - incomplete application\n",
    "    #\n",
    "    string_fmt = 'UTF-8' # 'ISO-8859-1'\n",
    "    \n",
    "    #\n",
    "    # flag whether to allow integer-float equivalence, e.g. 1.00 = 1 (yes) 1.0001 = 1 (no)\n",
    "    #\n",
    "    int_float_accept = False\n",
    "    \n",
    "    #\n",
    "    # filename modification of source file, when source csv is reconcontructed from netCDF\n",
    "    #\n",
    "    fname_ext = '_redo_'\n",
    "    \n",
    "    \n",
    "    #---------------------------------------\n",
    "    # Misc. specifications (in progress)\n",
    "    #---------------------------------------\n",
    "    \n",
    "    #\n",
    "    # logging options (optional flag - log to screen, set log level (e.g. debug, info, warning)) \n",
    "    #\n",
    "    log_screen = True\n",
    "    log_level = logging.DEBUG\n",
    "    \n",
    "    #\n",
    "    # used for testing only, limit processing to # datasets, default is false (off)\n",
    "    proc_limit = False \n",
    "\n",
    "    # note: prefilter *currently* used in dataframe method, postfilter works for dataframe or messytable methods\n",
    "\n",
    "    #\n",
    "    # flag whether to prefilter source csv file, filters by \"convert_chars\" + regex functions \n",
    "    #\n",
    "    prefilter = False\n",
    "\n",
    "    #\n",
    "    # flag whether to post-filter file reads during comparisons to allow differences, same technique as prefilter\n",
    "    #\n",
    "    postfilter = False\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # qoute interpreter, default, if None --> autoconfigured\n",
    "    #\n",
    "    quote_style = csv.QUOTE_NONNUMERIC # csv.QUOTE_MINIMAL\n",
    "    \n",
    "\n",
    "    # dict of character strings to filter from dataset (prefilter or postfilter)\n",
    "    # key, value : replaced term, modified term\n",
    "    # note: search keys are not case sensitive \n",
    "    convert_chars =  collections.OrderedDict([\n",
    "        ('\\n' , ''),\n",
    "        (',n/a,'  , ',' + absent_string + ','),\n",
    "        (',none,'  , ',NA,'),\n",
    "        (',na,'  , ',NA,') ])\n",
    "    \n",
    "    # Character string qoute format - used in Dataframe conversion method and file comparisons,\n",
    "    # note: dataframe method re-evaluates during NetCDF processing, \n",
    "    # inputs below are defaults, may be updated or overwritten internally\n",
    "    # \n",
    "    #\n",
    "    q_fmt = ['\"', '\"{}\"', \"'\"]\n",
    "    qoute_format = '''[\\,](?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)'''\n",
    "    \n",
    "    #\n",
    "    # Dictionary of ERDDAP reserved variables (L.L.A.T) specifications\n",
    "    #\n",
    "    LLAT_specs = dict([\n",
    "         ('longitude',{'destinationName':'longitude', 'units':'degrees_east'}),\n",
    "         ('latitude',{'destinationName':'latitude', 'units':'degrees_north'}),\n",
    "         ('altitude',{'destinationName':'altitude', 'units':'m', 'positive': 'up'}),\n",
    "         ('depth', {'destinationName':'depth', 'units':'m', 'positive': 'down'}),\n",
    "         ('eventDate',{'destinationName':'time'})  \n",
    "     ])\n",
    "    \n",
    "    #\n",
    "    # Search terms used on variable names, data type must be numeric or string \n",
    "    # (adhoc logic, customize as needed)\n",
    "    #\n",
    "    numeric_terms = ['_meters', 'minimum_', 'maximum_', '_minimum_', '_maximum', 'decimal', 'in_meters']\n",
    "    string_terms =  ['_id,', '_code']\n",
    "    \n",
    "    #\n",
    "    # Dictionary of partial variable terms to infer units (adhoc logic, customize as needed)\n",
    "    # key, value --> term, unit \n",
    "    variable_units = dict([\n",
    "         ('inKg','kg'),\n",
    "         ('in_meters','m'),\n",
    "         ('inmeters','m'),\n",
    "         ('WeightsN','N'),])\n",
    "  \n",
    "    # assemble commands dictionary - (arguments and options) \n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    commands = dict([('collection_id', collection_id),\n",
    "    ('file_srch', file_srch), \n",
    "    ('workdir', workdir),\n",
    "    ('source_data_dir', source_data_dir),\n",
    "    ('erddap_data_dir', erddap_data_dir),\n",
    "    ('server_nc_directory', server_nc_directory),\n",
    "    ('recon_data_dir', recon_data_dir), \n",
    "    ('report_dir', report_dir), \n",
    "    ('tempdir', tempdir), \n",
    "    ('darwin_vocab', darwin_vocab),\n",
    "    ('vocab_name', vocab_name),                \n",
    "    ('convert_method', convert_method), \n",
    "    ('fetch_metadata', fetch_metadata), \n",
    "    ('fetch_csvs', fetch_csvs), \n",
    "    ('create_netcdf_files', create_netcdf_files),\n",
    "    ('create_datasets_xml', create_datasets_xml), \n",
    "    ('file_overwrite', file_overwrite),\n",
    "    ('proc_overwrite', proc_overwrite),                 \n",
    "    ('compare_csv2csv', compare_csv2csv),\n",
    "    ('dump_csv', dump_csv), \n",
    "    ('error_report', error_report), \n",
    "    ('table_output', table_output), \n",
    "    ('max_report', max_report), \n",
    "    ('create_virtual_datasets', create_virtual_datasets),\n",
    "    ('verbose', verbose), \n",
    "    ('sample_size', sample_size), \n",
    "    ('window', window),\n",
    "    ('rows_per_file', rows_per_file), \n",
    "    ('int_float_accept', int_float_accept), \n",
    "    ('date_convert', date_convert),\n",
    "    ('numeric_terms', numeric_terms),                \n",
    "    ('string_terms', string_terms),                 \n",
    "    ('variable_units', variable_units),                 \n",
    "    ('date_fmt', date_fmt),  \n",
    "    ('string_fmt', string_fmt), \n",
    "    ('chunk_elements', chunk_elements),\n",
    "    ('cf_comply', cf_comply),\n",
    "    ('absent_string', absent_string), \n",
    "    ('netcdf_type', netcdf_type),                        \n",
    "    ('fname_ext', fname_ext),\n",
    "    ('log_screen', log_screen),\n",
    "    ('log_level', log_level), \n",
    "    ('proc_limit', proc_limit),                     \n",
    "    ('prefilter', prefilter), \n",
    "    ('postfilter', postfilter),\n",
    "    ('quote_style', quote_style),\n",
    "    ('convert_chars', convert_chars),\n",
    "    ('q_fmt',q_fmt),   \n",
    "    ('qoute_format', qoute_format),\n",
    "    ('LLAT_specs' , LLAT_specs),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaml file created in local directory\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "# create yaml config file\n",
    "#--------------------------------\n",
    "\n",
    "# retrieve commands dictionary (modify inputs above)\n",
    "commands = default_inputs() \n",
    "\n",
    "# Write *.YAML file\n",
    "with open('options.yaml', 'w', encoding='utf8') as outfile:\n",
    "    yaml.dump(commands, outfile, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "print('yaml file created in local directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_recent]",
   "language": "python",
   "name": "conda-env-py3_recent-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
